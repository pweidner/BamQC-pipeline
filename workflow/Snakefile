# Import necessary modules
import os
import glob
import subprocess
import yaml
import pandas as pd
from utils import extract_alignment_metrics

#ASCII art for BamQC-BIH
print(r"""


 /$$$$$$$                           /$$$$$$   /$$$$$$        /$$$$$$$  /$$$$$$ /$$   /$$
| $$__  $$                         /$$__  $$ /$$__  $$      | $$__  $$|_  $$_/| $$  | $$
| $$  \ $$  /$$$$$$  /$$$$$$/$$$$ | $$  \ $$| $$  \__/      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$  |____  $$| $$_  $$_  $$| $$  | $$| $$            | $$$$$$$   | $$  | $$$$$$$$
| $$__  $$  /$$$$$$$| $$ \ $$ \ $$| $$  | $$| $$            | $$__  $$  | $$  | $$__  $$
| $$  \ $$ /$$__  $$| $$ | $$ | $$| $$/$$ $$| $$    $$      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$/|  $$$$$$$| $$ | $$ | $$|  $$$$$$/|  $$$$$$/      | $$$$$$$/ /$$$$$$| $$  | $$
|_______/  \_______/|__/ |__/ |__/ \____ $$$ \______/       |_______/ |______/|__/  |__/
                                        \__/                                                       

""")

# Load config
configfile: "config/config.yaml"

# Define input and output directories
data_location = config["data_location"]
output_location = config["output_location"]
ref = config["ref"]
reference_path = config["reference_path"]

# Print summary of parameters
print("Pipeline launched with the following parameters:")
print(f"- Data Location: \033[1;31m{data_location}\033[0m")
print(f"- Output Location: \033[1;32m{output_location}\033[0m")
print(f"- Reference Genome: \033[1;33m{ref}\033[0m")
print("")

# Define the file path to the reference genomes
reference_file = os.path.join(reference_path, f"{ref}.fa")

# Define the list of samples based on the input BAM files
samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]

# Rule to generate a list of BAM files
rule all:
    """
    Collect all sample QC files as final outputs
    """ 
    input:    
        os.path.join(output_location, "alignment_summary_metrics.tsv"),
        os.path.join(output_location, "rl_metrics.tsv"),
        os.path.join(output_location, "bq_metrics.tsv"),
        os.path.join(output_location, "bc_metrics.tsv"),
        os.path.join(output_location, "mq_metrics.tsv"),
        os.path.join(output_location, "co_metrics.tsv"),
        os.path.join(output_location, "cm_metrics.tsv"),
        os.path.join(output_location, "is_metrics.tsv"),
        os.path.join(output_location, "ic_metrics.tsv"),
        os.path.join(output_location, "iz_metrics.tsv"),
        os.path.join(output_location, "cg_metrics.tsv"),
        os.path.join(output_location, "gc_metrics.tsv")


# Print the expand function to debug
print("Expanding input files with the following pattern:")
print(os.path.join(config['output_location'], "{sample}.qc.tsv.gz"))
print("")

# Rule to apply Alfred QC pipeline to each BAM file individually
rule alfred_qc:
    input:
        bam = os.path.join(data_location, "{sample}.sort.mdup.bam"),
        reference = reference_file
    output:
        qc = os.path.join(output_location, "stats-by-lib", "{sample}.qc.tsv.gz")
    conda:
        "envs/qc_env.yaml"
    shell:
        """
        alfred qc -r {input.reference} -o {output.qc} {input.bam}
        """

rule aggregate_alignment_metrics:
    input:
        expand(os.path.join(output_location, "stats-by-lib", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        summary = os.path.join(output_location, "alignment_summary_metrics.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    run:
        # Use zgrep and cut to extract alignment summary metrics into a temporary file
        tmp_file = output[0].replace(".tsv", "_tmp.tsv")
        zgrep_cut_cmd = f"zgrep ^ME {input} | cut -f 2- > {tmp_file}"
        shell(zgrep_cut_cmd)
        
        # Initialize a DataFrame to store aggregated metrics
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_alignment_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.summary), mode='a')

        # Remove the temporary file
        os.remove(tmp_file)

rule generate_metrics_tables:
    input:
        expand(os.path.join(output_location, "stats-by-lib", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        rl_metrics = os.path.join(output_location, "rl_metrics.tsv"),
        bq_metrics = os.path.join(output_location, "bq_metrics.tsv"),
        bc_metrics = os.path.join(output_location, "bc_metrics.tsv"),
        mq_metrics = os.path.join(output_location, "mq_metrics.tsv"),
        co_metrics = os.path.join(output_location, "co_metrics.tsv"),
        cm_metrics = os.path.join(output_location, "cm_metrics.tsv"),
        is_metrics = os.path.join(output_location, "is_metrics.tsv"),
        ic_metrics = os.path.join(output_location, "ic_metrics.tsv"),
        iz_metrics = os.path.join(output_location, "iz_metrics.tsv"),
        cg_metrics = os.path.join(output_location, "cg_metrics.tsv"),
        gc_metrics = os.path.join(output_location, "gc_metrics.tsv")
    params:
        header_written = False  # Flag to check if header has been written
    run:
        # Initialize dictionaries to store metrics data
        metrics = {
            "rl_metrics": [],
            "bq_metrics": [],
            "bc_metrics": [],
            "mq_metrics": [],
            "co_metrics": [],
            "cm_metrics": [],
            "is_metrics": [],
            "ic_metrics": [],
            "iz_metrics": [],
            "cg_metrics": [],
            "gc_metrics": []
        }
        
        
        # Initialize the flag to track whether the header has been written
        header_written = False
        
        # Iterate over input files
        for infile in input:
            with gzip.open(infile, 'rt') as f:
                # Read the content of the file
                content = f.read()
                # Split the content by lines
                lines = content.strip().split('\n')
                
                # Start appending data from the second line onwards (skipping the header)
                for line in lines[1:]:
                    # Append the line to the corresponding metric data list
                    metric_data[key].append(line)
        
        # Write metric data to output files
        for key, value in output.items():
            with open(value, 'w') as f:
                # Write the header only if it hasn't been written yet
                if not header_written:
                    f.write('\n'.join(metric_data[key]))
                    header_written = True
                else:
                    f.write('\n'.join(metric_data[key])
