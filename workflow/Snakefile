# Snakefile
configfile: "config/config.yaml"

# =========================== Imports & helpers ============================
import os, sys, glob, re
from pathlib import Path

def _bool(x, default=False):
    if x is None: return default
    if isinstance(x, bool): return x
    return str(x).lower() in ("1","true","yes","y","on")

def sanitize_suffix(x, default):
    if x is None:
        x = default
    x = str(x)
    # normalize Unicode whitespace (incl NBSP) and remove *all* surrounding whitespace
    x = x.replace("\u00A0", " ")
    x = re.sub(r"\s+", "", x)   # removes any whitespace characters anywhere
    return x

# -------------------- Read config with sensible defaults ------------------
DATA_ROOT   = Path(config["data_location"]).resolve()
OUT_ROOT    = Path(config["output_location"]).resolve()
WIN_SIZE    = int(config.get("window", 200000))
PLOT_TOGGLE = _bool(config.get("plot", False))

# If the user provides an explicit path to the fasta, use it *exactly* as given.
# This puts responsibility on the user to provide a correct path (with quoting
# in config if it has spaces). If not provided, fall back to old ref_name/ref_base behavior.
REF_FASTA_CONF = config.get("reference_fasta", None)
if REF_FASTA_CONF:
    # Use exactly what the user wrote (no stripping or normalization)
    REF_FASTA = str(REF_FASTA_CONF)
else:
    REF_NAME    = config.get("ref", "hg38")
    REF_BASE    = Path(config.get("reference_path", ".")).resolve()
    REF_FASTA   = str(REF_BASE/f"{REF_NAME}.fa")

WIN_SIZE    = int(config.get("window", 200000))
PLOT_TOGGLE = _bool(config.get("plot", False))

# Keep BAM_EXT simple: accept single string (user responsibility). If user gives list, join by comma.
_raw_bam_ext = config.get("bam_ext", ".sort.mdup.bam")
if isinstance(_raw_bam_ext, (list, tuple)):
    BAM_EXT = ",".join(str(x) for x in _raw_bam_ext)
else:
    BAM_EXT = str(_raw_bam_ext)
TMP_DIR     = str(config.get("tmp_dir", "/data/cephfs-1/scratch/groups/sanders/tmp"))

# Informational prints (help catch config mistakes)
print(f"[CONFIG] REF_FASTA = {REF_FASTA}", file=sys.stderr)
print(f"[CONFIG] BAM_EXT  = {BAM_EXT}", file=sys.stderr)

# Ashley defaults + overrides
ASH_DEFAULT = {
    "enabled": True,
    "bin": "/data/cephfs-1/home/users/pweidne_m/work/ashleys-qc/bin/ashleys.py",
    "model_path": "/data/cephfs-1/home/users/pweidne_m/work/ashleys-qc/models/svc_default.pkl",
    "win_sizes": [5_000_000, 2_000_000, 1_000_000, 800_000, 600_000, 400_000, 200_000],
    "threads": 32,
    "mem_mb": 200000,
    "conda_env": "envs/ashleys.yaml",
}
ASH = {**ASH_DEFAULT, **config.get("ashleys", {})}
ASH_ENABLED = _bool(ASH.get("enabled", True))

# -------------------------- Discover BAMs (FLAT / HIER via find) --------------------------
def discover_bams(root: Path, bam_ext: str):
    """
    Discover BAMs using 'find' in two layouts:
      - FLAT: <root>/*<ext>            (maxdepth 1)
      - HIER: <root>/*/bam/*<ext>      (search each child/<bam> with maxdepth 1)

    Returns (mode, bam_of) where bam_of maps LibraryID -> absolute bam path.

    Notes:
    - Accepts comma-separated bam_ext (e.g. ".sort.mdup.bam,.mdup.bam,.bam").
    - Preserves basename suffixes like ".sort.mdup" (we only strip the final ".bam").
    """
    import subprocess, shlex

    def strip_only_trailing_bam(name: str) -> str:
        # remove only a trailing ".bam" if present, keep other dots (e.g. ".sort.mdup")
        if name.endswith(".bam"):
            return name[:-4]
        return name

    def sanitize_for_wildcard(s: str) -> str:
        # minimal sanitization to produce safe Snakemake wildcards but preserve name shape
        s = s.replace("\u00A0", " ")
        s = s.strip()
        s = re.sub(r"\s+", "_", s)
        # keep dot and dash and underscore and alnum
        s = re.sub(r"[^A-Za-z0-9._-]+", "_", s)
        s = re.sub(r"_+", "_", s)
        return s.strip("_")

    def run_find(path: str, maxdepth: int, name_pattern: str):
        cmd = ["find", path, "-maxdepth", str(maxdepth), "-type", "f", "-name", name_pattern]
        try:
            proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)
            if proc.stderr:
                # don't fail; just log
                print(f"[discover_bams] find stderr: {proc.stderr.strip()}", file=sys.stderr)
            lines = [l for l in proc.stdout.splitlines() if l.strip()]
            return [Path(l) for l in sorted(lines)]
        except Exception as e:
            print(f"[discover_bams] find invocation failed: {e}", file=sys.stderr)
            return []

    # quick check
    if not root.exists():
        print(f"[ERROR] data_location does not exist: {root}", file=sys.stderr)
        return None, {}

    # Parse bam_ext (comma-separated allowed)
    exts = [x.strip() for x in str(bam_ext or "").split(",") if x.strip()]
    if not exts:
        exts = [".sort.mdup.bam"]

    # normalize ext to start with a dot
    exts = [e if e.startswith(".") else "." + e for e in exts]

    # ---------- FLAT: maxdepth 1 ----------
    for ext in exts:
        pattern = f"*{ext}"           # <-- CAREFUL: no space between * and ext
        found = run_find(str(root), maxdepth=1, name_pattern=pattern)
        if found:
            bam_of = {}
            for p in found:
                base = p.name
                sid_raw = strip_only_trailing_bam(base)   # preserves .sort.mdup etc.
                sid = sanitize_for_wildcard(sid_raw)
                if sid in bam_of:
                    # rare duplicate: append short hash
                    sid = f"{sid}_{str(abs(hash(str(p))))[:6]}"
                bam_of[sid] = str(p.resolve())
            print(f"[discover_bams] FLAT: found {len(bam_of)} bams (ext={ext})", file=sys.stderr)
            print(f"[discover_bams] FLAT examples: {list(bam_of.keys())[:8]}", file=sys.stderr)
            return "FLAT", bam_of

    # ---------- HIER: look for <child>/bam/*<ext> (child maxdepth 2 search) ----------
    bam_of = {}
    for child in sorted(p for p in root.iterdir() if p.is_dir()):
        bamdir = child / "bam"
        if not bamdir.is_dir():
            continue
        for ext in exts:
            pattern = f"*{ext}"
            found = run_find(str(bamdir), maxdepth=1, name_pattern=pattern)
            for p in found:
                base = p.name
                bam_base = strip_only_trailing_bam(base)   # e.g. A5573_L1_i301.sort.mdup
                sid_raw = f"{child.name}_{bam_base}"
                sid = sanitize_for_wildcard(sid_raw)
                if sid in bam_of:
                    sid = f"{sid}_{str(abs(hash(str(p))))[:6]}"
                bam_of[sid] = str(p.resolve())

    if bam_of:
        print(f"[discover_bams] HIER: found {len(bam_of)} bams under child/*/bam", file=sys.stderr)
        print(f"[discover_bams] HIER examples: {list(bam_of.keys())[:8]}", file=sys.stderr)
        return "HIER", bam_of

    # nothing found
    print(f"[ERROR] No BAMs found under {root}. Searched extensions: {exts}", file=sys.stderr)
    print("[ERROR] Expected FLAT: <data_root>/*<ext> OR HIER: <data_root>/<sample>/bam/*<ext>", file=sys.stderr)
    return None, {}


MODE, BAM_OF = discover_bams(DATA_ROOT, BAM_EXT)
SAMPLES = sorted(BAM_OF.keys())

# ================================= ASCII Art =================================
print(r"""
 /$$                                                                    /$$                     /$$ /$$                                       /$$$$$$       /$$  
| $$                                                                   |__/                    | $$|__/                                      /$$$_  $$    /$$$$  
| $$$$$$$   /$$$$$$  /$$$$$$/$$$$   /$$$$$$   /$$$$$$$         /$$$$$$  /$$  /$$$$$$   /$$$$$$ | $$ /$$ /$$$$$$$   /$$$$$$        /$$    /$$| $$$$\ $$   |_  $$  
| $$__  $$ |____  $$| $$_  $$_  $$ /$$__  $$ /$$_____//$$$$$$ /$$__  $$| $$ /$$__  $$ /$$__  $$| $$| $$| $$__  $$ /$$__  $$      |  $$  /$$/| $$ $$ $$     | $$  
| $$  \ $$  /$$$$$$$| $$ \ $$ \ $$| $$  \ $$| $$     |______/| $$  \ $$| $$| $$  \ $$| $$$$$$$$| $$| $$| $$  \ $$| $$$$$$$$       \  $$/$$/ | $$\ $$$$     | $$  
| $$  | $$ /$$__  $$| $$ | $$ | $$| $$  | $$| $$             | $$  | $$| $$| $$  | $$| $$_____/| $$| $$| $$  | $$| $$_____/        \  $$$/  | $$ \ $$$     | $$  
| $$$$$$$/|  $$$$$$$| $$ | $$ | $$|  $$$$$$$|  $$$$$$$       | $$$$$$$/| $$| $$$$$$$/|  $$$$$$$| $$| $$| $$  | $$|  $$$$$$$         \  $/   |  $$$$$$//$$ /$$$$$$
|_______/  \_______/|__/ |__/ |__/ \____  $$ \_______/       | $$____/ |__/| $$____/  \_______/|__/|__/|__/  |__/ \_______/          \_/     \______/|__/|______/
                                        | $$                 | $$          | $$                                                                                  
                                        | $$                 | $$          | $$                                                                                  
                                        |__/                 |__/          |__/                                                                                  
""")
print("Starting BamQC pipeline...\n")

print("\n=== BamQC: input discovery ===")
print(f"- Mode: {MODE}")
print(f"- data_location: {DATA_ROOT}")
print(f"- output_location: {OUT_ROOT}")
print(f"- reference: {REF_NAME} ({REF_FASTA})")
print(f"- discovered libraries: {len(SAMPLES)}")
print("  · showing first 8:", ", ".join(SAMPLES[:8]), ("..." if len(SAMPLES)>8 else ""))
print(f"- Ashley: {'ENABLED' if ASH_ENABLED else 'disabled'}")
print("================================\n")

# ---------------------------- Output structure ----------------------------
def out_path(*parts): return str(OUT_ROOT.joinpath(*parts))

DIR_LOGS   = out_path("logs")
DIR_STATS  = out_path("stats-by-lib")
DIR_BINS   = out_path("binned")
DIR_QC2    = out_path("qc-from-bins")
DIR_PRESEQ = out_path("preseq")
DIR_PLOTS  = out_path("plots")

# new: clearly discoverable results + metadata
DIR_RESULTS  = out_path("results")
DIR_METADATA = out_path("metadata")

WIN_BED_GZ = out_path(f"windows_{WIN_SIZE}.bed.gz")
WIN_GC_GZ  = out_path(f"windows_{WIN_SIZE}.gc.tsv.gz")

# Ashley
DIR_ASH        = out_path("ashleys")
DIR_ASH_LOGS   = out_path("ashleys", "logs")
ASH_FEATURES   = out_path("ashleys", "features.tsv")
ASH_FEAT_NORM  = out_path("ashleys", "features.norm.tsv")
ASH_PRED_DIR   = out_path("ashleys", "prediction")
ASH_PRED_TSV   = out_path("ashleys", "prediction", "prediction.tsv")
ASH_PRED_NORM  = out_path("ashleys", "prediction", "prediction.norm.tsv")

# metadata
LIBMAP_TSV     = out_path("metadata", "library_map.tsv")

# Helper for Ashley window sizes
def ash_win_str():
    return " ".join(str(w) for w in ASH.get("win_sizes", ASH_DEFAULT["win_sizes"]))

# ============================== RULE ALL =====================================
plot_targets = []
if PLOT_TOGGLE:
    plot_targets = expand(out_path("plots", "per-lib-qc", "{sample}.qc.pdf"), sample=SAMPLES)

ALL_TARGETS = [
    out_path("alignment_summary_metrics.tsv"),
    out_path("final_qc.tsv"),
    out_path("plots", "run_summary.pdf"),
    # always build preseq curves so the dir is populated
    *expand(out_path("preseq", "{sample}.lc.tsv"), sample=SAMPLES),
    *plot_targets
]
if ASH_ENABLED:
    ALL_TARGETS.append(out_path("final_qc_with_ashleys.tsv"))

rule all:
    input: ALL_TARGETS

# ============================== CORE RULES ===================================
rule alfred_qc:
    input:
        bam=lambda wc: BAM_OF[wc.sample],
        ref=REF_FASTA
    output:
        qc=out_path("stats-by-lib", "{sample}.qc.tsv.gz")
    conda: "envs/qc_env.yaml"
    threads: 4
    log:
        out_path("logs", "{sample}.alfred_qc.log")
    params:
        log_dir=DIR_LOGS,
        stats_dir=DIR_STATS
    shell:
        """
        mkdir -p {params.log_dir} {params.stats_dir}
        echo "Processing sample: {wildcards.sample}" > {log}
        alfred qc -r {input.ref} -o {output.qc} {input.bam} >> {log} 2>&1
        """

rule aggregate_alignment_metrics:
    input:
        qc_files=expand(out_path("stats-by-lib", "{sample}.qc.tsv.gz"), sample=SAMPLES)
    output:
        summary=out_path("alignment_summary_metrics.tsv")
    threads: 2
    run:
        import os, gzip, tempfile, pandas as pd
        odir = os.path.dirname(output.summary)
        os.makedirs(odir, exist_ok=True)

        records = []
        for qc in input.qc_files:
            sample_id = os.path.basename(qc).replace(".qc.tsv.gz", "")
            header, values = None, None
            with gzip.open(qc, 'rt') as fh:
                for line in fh:
                    if line.startswith("ME"):
                        toks = line.rstrip("\n").split("\t")
                        if header is None:
                            header = toks[1:]
                        else:
                            values = toks[1:]
                            break
            if header and values and len(header) == len(values):
                rec = dict(zip(header, values))
                rec["Library"] = sample_id
                records.append(rec)
            else:
                print(f"[aggregate_alignment_metrics] Warning: could not parse 'ME' lines in {qc}")

        import pandas as pd
        df = pd.DataFrame.from_records(records)
        if "Library" in df.columns:
            cols = ["Library"] + [c for c in df.columns if c != "Library"]
            df = df[cols]

        with tempfile.NamedTemporaryFile("w", delete=False, dir=odir, suffix=".tmp") as tf:
            tmp_path = tf.name
            df.to_csv(tf, sep="\t", index=False)
            tf.flush(); os.fsync(tf.fileno())
        os.replace(tmp_path, output.summary)

rule make_windows:
    input:
        fai=f"{REF_FASTA}.fai"
    output:
        bed=WIN_BED_GZ
    conda: "envs/qc_env.yaml"
    params:
        out_dir=str(OUT_ROOT),
        window=WIN_SIZE
    shell:
        """
        mkdir -p {params.out_dir}
        bedtools makewindows -g {input.fai} -w {params.window} \
          | bgzip -c > {output.bed}
        """

rule coverage_counts:
    input:
        bam=lambda wc: BAM_OF[wc.sample],
        bai=lambda wc: BAM_OF[wc.sample] + ".bai",
        win=WIN_BED_GZ
    output:
        tsv=out_path("binned", "{sample}.bins.tsv.gz")
    threads: 2
    resources:
        mem_mb=20000,
        partition="medium",
        qos="normal",
        time="04:00:00",
        tmpdir=TMP_DIR
    conda: "envs/py_env.yaml"
    log:
        out_path("logs", "{sample}.coverage_counts.log")
    params:
        bins_dir=DIR_BINS,
        log_dir=DIR_LOGS
    shell:
        r"""
        set -euo pipefail
        mkdir -p {params.bins_dir} {params.log_dir} "{resources.tmpdir}"

        tmpdir="{resources.tmpdir}"
        fprefix="{wildcards.sample}"
        fbam="${{tmpdir}}/${{fprefix}}.chr1_22_XY.filtered.bam"
        gfile="${{tmpdir}}/${{fprefix}}.genome"
        tbed="${{tmpdir}}/${{fprefix}}.chr1_22_XY.targets.bed"
        wbed="{input.win}"

        # Build genome file (chrom\tlength)
        samtools view -H {input.bam} \
          | awk 'BEGIN{{OFS="\t"}} $1=="@SQ"{{split($2,a,":"); split($3,b,":"); print a[2], b[2]}}' > "${{gfile}}"

        bam_has_chr=$(head -n1 "${{gfile}}" | cut -f1 | grep -c '^chr' || true)

        if [[ "${{bam_has_chr}}" -eq 1 ]]; then
          keep=$(printf 'chr%s\n' $(seq 1 22) X Y)
        else
          keep=$(printf '%s\n' $(seq 1 22) X Y)
        fi

        awk 'BEGIN{{OFS="\t"}} FNR==NR{{keep[$1]=1; next}} ($1 in keep){{print $1,0,$2}}' \
          <(printf "%s\n" $keep) "${{gfile}}" > "${{tbed}}"

        samtools view -@ {threads} -b -F 3584 -q 10 -L "${{tbed}}" {input.bam} -o "${{fbam}}"
        samtools index -@ {threads} "${{fbam}}"

        zcat "${{wbed}}" \
          | awk -v bam_has_chr="${{bam_has_chr}}" 'BEGIN{{OFS="\t"}}
                {{ c=$1; s=$2; e=$3;
                   if (bam_has_chr==1 && c !~ /^chr/) c="chr" c;
                   else if (bam_has_chr==0 && c ~ /^chr/) sub(/^chr/,"",c);
                   print c,s,e }}' \
          | awk 'BEGIN{{OFS="\t"}} FNR==NR{{len[$1]=$2; next}} ($1 in len){{print $0}}' "${{gfile}}" - \
          | bedtools sort -g "${{gfile}}" -i - \
          | bgzip -c > "${{wbed}}.harmonized.{wildcards.sample}.bed.gz"

        bedtools coverage \
          -sorted \
          -g "${{gfile}}" \
          -a "${{wbed}}.harmonized.{wildcards.sample}.bed.gz" \
          -b "${{fbam}}" \
          -counts \
          2>> {log} \
          | bgzip -c > {output.tsv}

        rm -f "${{fbam}}" "${{fbam}}.bai" "${{wbed}}.harmonized.{wildcards.sample}.bed.gz" "${{tbed}}" "${{gfile}}"
        """

rule windows_gc:
    input:
        win=WIN_BED_GZ,
        fa=REF_FASTA
    output:
        gc=WIN_GC_GZ
    conda: "envs/qc_env.yaml"
    log:
        out_path("logs", f"windows_{WIN_SIZE}.gc.log")
    params:
        out_dir=str(OUT_ROOT)
    shell:
        r"""
        set -euo pipefail
        mkdir -p {params.out_dir} $(dirname {log})
        (
          echo -e "chrom\tstart\tend\tGC"
          zcat {input.win} \
            | bedtools nuc -fi {input.fa} -bed - \
            | awk 'BEGIN{{OFS="\t"}} NR>1 {{print $1,$2,$3,$5}}'
        ) | bgzip -c > {output.gc} 2> {log}
        """

rule preseq_lc:
    input:
        bam=lambda wc: BAM_OF[wc.sample],
        bai=lambda wc: BAM_OF[wc.sample] + ".bai"
    output:
        tsv=out_path("preseq", "{sample}.lc.tsv")
    threads: 4
    resources:
        mem_mb=8000,
        partition="short",
        qos="normal",
        time="01:30:00",
        tmpdir=TMP_DIR
    conda: "envs/preseq.yaml"
    log:
        out_path("logs", "{sample}.preseq.log")
    params:
        preseq_dir=DIR_PRESEQ,
        log_dir=DIR_LOGS
    shell:
        r"""
        # Snakemake's jobscript uses: set -euo pipefail
        # → we MUST explicitly disable -e and pipefail here.
        set +e
        set +o pipefail
        set -u

        mkdir -p {params.preseq_dir} {params.log_dir} {resources.tmpdir}
        wdir="{resources.tmpdir}/{wildcards.sample}.preseq"
        mkdir -p "${{wdir}}"
        pbam="${{wdir}}/1.primary.bam"
        nbam="${{wdir}}/2.namesort.bam"
        bedpe="${{wdir}}/3.pairs.bedpe"
        frag="${{wdir}}/4.fragments.bed"
        frgs="${{wdir}}/5.fragments.sorted.bed"

        echo "[which]" >> {log}
        (command -v samtools bedtools preseq >> {log}) || echo "[which] command -v failed" >> {log}
        preseq 2>&1 | head -n 3 >> {log} || echo "[preseq] version call failed" >> {log}

        samtools view -@ {threads} -b -F 2304 {input.bam} -o "${{pbam}}" >> {log} 2>&1 || echo "[ERROR] samtools view failed" >> {log}
        samtools sort -@ {threads} -n -o "${{nbam}}" "${{pbam}}" >> {log} 2>&1 || echo "[ERROR] samtools sort failed" >> {log}
        bedtools bamtobed -bedpe -i "${{nbam}}" > "${{bedpe}}" 2>> {log} || echo "[ERROR] bedtools bamtobed failed" >> {log}

        awk 'BEGIN{{OFS="\t"}} $1==$4 && $2!=-1 && $5!=-1 {{ s=($2<$5)?$2:$5; e=($3>$6)?$3:$6; if(e>s) print $1,s,e; }}' \
            "${{bedpe}}" > "${{frag}}" || echo "[ERROR] awk fragment generation failed" >> {log}

        LC_ALL=C sort -k1,1 -k2,2n -k3,3n "${{frag}}" > "${{frgs}}" || echo "[ERROR] sort fragments failed" >> {log}

        if [[ -s "${{frgs}}" ]]; then
          maxdup=$(awk 'BEGIN{{FS=OFS="\t"}} {{k=$1":"$2"-"$3; c[k]++}} END{{m=0; for(k in c) if(c[k]>m) m=c[k]; print m+0}}' "${{frgs}}")
          nfrag=$(wc -l < "${{frgs}}")
          echo "[dup_spectrum] fragments=${{nfrag}} max_dup=${{maxdup}}" >> {log}

          if [[ "${{nfrag}}" -lt 1000 ]]; then
            echo "[preseq] Too few fragments (<1000). Using -D -Q fallback." >> {log}
            preseq lc_extrap -D -Q -o {output.tsv} "${{frgs}}" >> {log} 2>&1 || echo "[preseq] lc_extrap -D -Q failed" >> {log}
          elif [[ "${{maxdup}}" -lt 4 ]]; then
            echo "[preseq] Sparse duplicate spectrum (max_dup<4). Using -D -Q fallback." >> {log}
            preseq lc_extrap -D -Q -o {output.tsv} "${{frgs}}" >> {log} 2>&1 || echo "[preseq] lc_extrap -D -Q failed" >> {log}
          else
            echo "[preseq] Running standard lc_extrap." >> {log}
            preseq lc_extrap -o {output.tsv} "${{frgs}}" >> {log} 2>&1 || echo "[preseq] standard lc_extrap failed" >> {log}
          fi
        else
          echo "[preseq] No fragments produced – skipping lc_extrap." >> {log}
        fi

        # SAFETY NET: always produce something
        if [[ ! -s "{output.tsv}" ]]; then
          echo -e "TOTAL_READS\tEXPECTED_DISTINCT\tLOWER_0.95CI\tUPPER_0.95CI" > {output.tsv}
          echo -e "0\t0\t0\t0" >> {output.tsv}
          echo "[preseq] Wrote stub curve due to failure/sparsity." >> {log}
        fi

        rm -rf "${{wdir}}"
        """

rule qc_from_counts:
    input:
        tsv=out_path("binned", "{sample}.bins.tsv.gz"),
        gc=WIN_GC_GZ,
        preseq=out_path("preseq", "{sample}.lc.tsv"),
        alfred=out_path("alignment_summary_metrics.tsv")
    output:
        qc=out_path("qc-from-bins", "{sample}.counts_qc.tsv")
    conda: "envs/py_env.yaml"
    threads: 1
    resources:
        mem_mb=4000,
        partition="short",
        qos="normal",
        time="01:30:00",
        tmpdir=TMP_DIR
    log:
        out_path("logs", "{sample}.qc_from_counts.log")
    params:
        qc2_dir=DIR_QC2,
        log_dir=DIR_LOGS
    shell:
        r"""
        set -euo pipefail
        mkdir -p {params.qc2_dir} {params.log_dir}
        python -u workflow/scripts/qc_from_counts.py \
          --counts {input.tsv} \
          --sample {wildcards.sample} \
          --gc-table {input.gc} \
          --alfred-summary {input.alfred} \
          --preseq {input.preseq} \
          --out {output.qc} > {log} 2>&1
        """

rule make_sample_final_qc:
    input:
        alfred=out_path("alignment_summary_metrics.tsv"),
        qc2=expand(out_path("qc-from-bins", "{sample}.counts_qc.tsv"), sample=SAMPLES)
    output:
        final=out_path("final_qc.tsv")
    threads: 2
    resources:
        mem_mb=20000,
        partition="medium",
        qos="normal",
        time="02:00:00"
    run:
        import os, re, tempfile, pandas as pd

        def snake(s: str) -> str:
            s = s.strip()
            s = s.replace(".", "_")
            s = re.sub(r"[^A-Za-z0-9_]+", "_", s)
            s = re.sub(r"__+", "_", s)
            return s.strip("_").lower()

        # --- Alfred rename map (explicit; deterministic) ---
        alf_map = {
            "#QCFail": "alf_qcfail_n",
            "QCFailFraction": "alf_qcfail_frac",
            "#DuplicateMarked": "alf_duplicate_marked_n",
            "DuplicateFraction": "alf_duplicate_frac",
            "#Unmapped": "alf_unmapped_n",
            "UnmappedFraction": "alf_unmapped_frac",
            "#Mapped": "alf_mapped_n",
            "MappedFraction": "alf_mapped_frac",
            "#MappedRead1": "alf_mapped_read1_n",
            "#MappedRead2": "alf_mapped_read2_n",
            "RatioMapped2vsMapped1": "alf_mapped2_vs_mapped1_ratio",
            "#MappedForward": "alf_mapped_forward_n",
            "MappedForwardFraction": "alf_mapped_forward_frac",
            "#MappedReverse": "alf_mapped_reverse_n",
            "MappedReverseFraction": "alf_mapped_reverse_frac",
            "#SecondaryAlignments": "alf_secondary_alignments_n",
            "SecondaryAlignmentFraction": "alf_secondary_alignments_frac",
            "#SupplementaryAlignments": "alf_supplementary_alignments_n",
            "SupplementaryAlignmentFraction": "alf_supplementary_alignments_frac",
            "#SplicedAlignments": "alf_spliced_alignments_n",
            "SplicedAlignmentFraction": "alf_spliced_alignments_frac",
            "#Pairs": "alf_pairs_n",
            "#MappedPairs": "alf_mapped_pairs_n",
            "MappedPairsFraction": "alf_mapped_pairs_frac",
            "#MappedSameChr": "alf_mapped_same_chr_n",
            "MappedSameChrFraction": "alf_mapped_same_chr_frac",
            "#MappedProperPair": "alf_mapped_proper_pair_n",
            "MappedProperFraction": "alf_mapped_proper_pair_frac",
            "#ReferenceBp": "alf_reference_bp",
            "#ReferenceNs": "alf_reference_ns",
            "#AlignedBases": "alf_aligned_bp",
            "#MatchedBases": "alf_matched_bp",
            "MatchRate": "alf_match_frac",
            "#MismatchedBases": "alf_mismatched_bp",
            "MismatchRate": "alf_mismatch_frac",
            "#DeletionsCigarD": "alf_deletions_d_n",
            "DeletionRate": "alf_deletion_frac",
            "HomopolymerContextDel": "alf_homopolymer_context_del",
            "#InsertionsCigarI": "alf_insertions_i_n",
            "InsertionRate": "alf_insertion_frac",
            "HomopolymerContextIns": "alf_homopolymer_context_ins",
            "#SoftClippedBases": "alf_soft_clipped_bp",
            "SoftClipRate": "alf_soft_clip_frac",
            "#HardClippedBases": "alf_hard_clipped_bp",
            "HardClipRate": "alf_hard_clip_frac",
            "ErrorRate": "alf_error_frac",
            "MedianReadLength": "alf_read_length_med",
            "DefaultLibraryLayout": "alf_library_layout_default",
            "MedianInsertSize": "alf_insert_size_med",
            "MedianCoverage": "alf_coverage_med",
            "SDCoverage": "alf_coverage_sd",
            "CoveredBp": "alf_covered_bp",
            "FractionCovered": "alf_covered_frac",
            "BpCov1ToCovNRatio": "alf_bp_cov1_to_covn_ratio",
            "BpCov1ToCov2Ratio": "alf_bp_cov1_to_cov2_ratio",
            "MedianMAPQ": "alf_mapq_med",
        }

        # --- Bin qc rename map (explicit; deterministic) ---
        bin_map = {
            "n.bins": "bin_n_bins",
            "avg.binsize": "bin_avg_binsize",
            "total.read.count": "bin_total_read_count",
            "avg.read.count": "bin_avg_read_count",
            "spikiness": "bin_spikiness",
            "entropy": "bin_entropy",
            "coverage_gini": "bin_gini",
            "coverage_cv": "bin_cv",
            "coverage_mad": "bin_mad",
            "coverage_sd": "bin_sd",
            "fold80_penalty": "bin_fold80",
            "gc_pearson_r": "bin_gc_r",
            "pct_ge_1x": "bin_pct_ge_1x",
            "pct_ge_10x": "bin_pct_ge_10x",
            "pct_ge_30x": "bin_pct_ge_30x",
            # Keep preseq fields as-is (already your chosen prefix)
            "preseq_distinct_at_observed": "preseq_distinct_at_observed",
            "preseq_saturation": "preseq_saturation",
        }

        # Read inputs
        dfA = pd.read_csv(input.alfred, sep="\t")
        rows = [pd.read_csv(p, sep="\t") for p in input.qc2]
        dfQ = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=["Library"])

        # Ensure keys exist
        if "Library" not in dfA.columns:
            raise ValueError("[make_sample_final_qc] alignment_summary_metrics.tsv lacks 'Library'")
        if "Sample" not in dfA.columns:
            # If Sample ever disappears from Alfred ME extraction, keep pipeline alive
            dfA["Sample"] = dfA["Library"].astype(str).str.replace(r"_A\d+_L\d+_i\d+.*$", "", regex=True)

        # Rename Alfred columns (keep Library/Sample)
        a_ren = {}
        for c in dfA.columns:
            if c in ("Library", "Sample"):
                continue
            if c in alf_map:
                a_ren[c] = alf_map[c]
            else:
                # fallback: prefix and snake-case unknown alfred columns
                a_ren[c] = "alf_" + snake(c)
        dfA = dfA.rename(columns=a_ren)

        # Drop redundant duplicates from bin table if still present
        # (after you patch qc_from_counts.py, these should be gone)
        for redundant in ("MedianInsertSize", "MedianMAPQ"):
            if redundant in dfQ.columns:
                dfQ = dfQ.drop(columns=[redundant])

        # Rename bin columns (keep Library)
        q_ren = {}
        for c in dfQ.columns:
            if c == "Library":
                continue
            if c in bin_map:
                q_ren[c] = bin_map[c]
            else:
                # fallback: prefix + snake-case
                # if it already starts with preseq_, keep it stable
                if c.startswith("preseq_"):
                    q_ren[c] = snake(c)  # enforce snake
                else:
                    q_ren[c] = "bin_" + snake(c)
        dfQ = dfQ.rename(columns=q_ren)

        # Merge
        merged = dfA.merge(dfQ, on="Library", how="left")

        # Column order: IDs first
        id_cols = ["Library", "Sample"]
        other_cols = [c for c in merged.columns if c not in id_cols]
        merged = merged[id_cols + other_cols]

        # Atomic write
        odir = os.path.dirname(output.final)
        os.makedirs(odir, exist_ok=True)
        with tempfile.NamedTemporaryFile("w", delete=False, dir=odir, suffix=".tmp") as tf:
            tmp = tf.name
            merged.to_csv(tf, sep="\t", index=False)
            tf.flush(); os.fsync(tf.fileno())
        os.replace(tmp, output.final)


# --------------------------- OPTIONAL PLOTTING -------------------------------
rule plot_qc:
    input:
        final=out_path("final_qc.tsv"),
        preseq=lambda wc: out_path("preseq", f"{wc.sample}.lc.tsv")
    output:
        pdf=out_path("plots", "per-lib-qc", "{sample}.qc.pdf")
    threads: 1
    resources:
        mem_mb=4000,
        partition="short",
        qos="normal",
        time="00:20:00",
        tmpdir=TMP_DIR
    conda: "envs/py_env.yaml"
    log:
        out_path("logs", "{sample}.plot_qc.log")
    params:
        plots_dir=DIR_PLOTS,
        preseq_dir=DIR_PRESEQ
    shell:
        r"""
        set -euo pipefail
        mkdir -p {params.plots_dir} $(dirname {log}) "{resources.tmpdir}"

        # Headless + safe font cache
        export MPLBACKEND=Agg
        export MPLCONFIGDIR="{resources.tmpdir}/{wildcards.sample}.mplcfg"
        mkdir -p "$MPLCONFIGDIR"

        # Optional: avoids numba trying to JIT on some clusters
        export NUMBA_DISABLE_JIT=1

        python -u workflow/scripts/plot_qc.py \
          --final {input.final} \
          --preseq_dir {params.preseq_dir} \
          --outdir {params.plots_dir} \
          --only {wildcards.sample} > {log} 2>&1
        """

rule plot_run_summary_R:
    input:
        final=out_path("final_qc.tsv")
    output:
        pdf=out_path("plots", "run_summary.pdf")
    threads: 1
    resources:
        mem_mb=4000,
        partition="short",
        qos="normal",
        time="00:20:00"
    conda: "envs/r_plot.yaml"
    log:
        out_path("logs", "run_summary_R.log")
    params:
        count_col="total.read.count"
    shell:
        r"""
        mkdir -p "$(dirname {log})" "$(dirname {output.pdf})"
        Rscript workflow/scripts/plot_summary.R \
          --final {input.final} \
          --out {output.pdf} \
          --count-col '{params.count_col}' \
          > {log} 2>&1
        """

# ==================== ASHLEYS-QC (auto-detect / normalize) ===================

# Build a mapping from BAM basename ("cell") → Library (our sample id)
rule write_library_map:
    output: LIBMAP_TSV
    run:
        import os, pandas as pd
        os.makedirs(os.path.dirname(output[0]), exist_ok=True)
        rows = []
        for lib, bam in BAM_OF.items():
            cell = os.path.basename(bam)  # e.g. A5455_L2_1001.sort.mdup.bam
            rows.append({"cell": cell, "Library": lib})
        pd.DataFrame(rows).to_csv(output[0], sep="\t", index=False)

# Merge existing Ashley features or compute them; then normalize to Library
rule ashleys_labels_or_features:
    output:
        features = ASH_FEATURES
    threads: ASH.get("threads", 32)
    resources:
        mem_mb = ASH.get("mem_mb", 200000),
        partition = "highmem",
        qos = "normal",
        time = "1-00:00:00"
    conda: ASH.get("conda_env", "envs/ashleys.yaml")
    log:
        out_path("ashleys", "logs", "features_or_labels.log")
    params:
        folder   = str(DATA_ROOT),
        windows  = ash_win_str(),
        extension= BAM_EXT,
        bin      = ASH.get("bin", ASH_DEFAULT["bin"])
    shell:
        r"""
        set -euo pipefail
        mkdir -p "$(dirname {log})" "$(dirname {output.features})"

        RUN_DIR="{config[data_location]}"
        MODE="FLAT"
        if find "$RUN_DIR" -mindepth 2 -maxdepth 2 -type d -name bam -print -quit | grep -q .; then
          MODE="HIER"
        fi

        # Prefer merging any precomputed features if present
        if [[ "$MODE" == "HIER" ]] && \
           find "$RUN_DIR" -type f \( -path '*/predictions/ashleys_features.tsv' -o -path '*/prediction/ashleys_features.tsv' -o -path '*/predicitons/ashleys_features.tsv' \) -print -quit | grep -q .; then

          TMP="{output.features}.tmp"
          find "$RUN_DIR" -type f \
            \( -path '*/predictions/ashleys_features.tsv' -o -path '*/prediction/ashleys_features.tsv' -o -path '*/predicitons/ashleys_features.tsv' \) \
            -print | sort \
            | xargs awk 'FNR==1 && NR>1 {{next}} {{print}}' > "$TMP"

          test -s "$TMP"
          mv -f "$TMP" "{output.features}"
        else
          # Compute features from BAMs
          python -u "{params.bin}" -j {threads} features \
            -f "{params.folder}" \
            -w {params.windows} \
            -o "{output.features}" \
            --recursive_collect \
            -e "{params.extension}" >> "{log}" 2>&1

          test -s "{output.features}"
        fi
        """

# Merge labels (if any) or predict from features, then normalize to Library
rule ashleys_predict_or_copy:
    input:
        features = rules.ashleys_labels_or_features.output.features
    output:
        pred_dir = directory(ASH_PRED_DIR),
        pred     = ASH_PRED_TSV
    threads: max(1, ASH.get("threads", 32)//2)
    resources:
        mem_mb = max(20000, ASH.get("mem_mb", 200000)//2),
        partition = "medium",
        qos = "normal",
        time = "04:00:00"
    conda: ASH.get("conda_env", "envs/ashleys.yaml")
    log:
        out_path("ashleys", "logs", "predict_or_copy.log")
    params:
        bin   = ASH.get("bin", ASH_DEFAULT["bin"]),
        model = ASH.get("model_path", ASH_DEFAULT["model_path"])
    shell:
        r"""
        set -euo pipefail
        RUN_DIR="{config[data_location]}"
        mkdir -p "{output.pred_dir}" "$(dirname {log})"

        # Try to merge any labels.tsv produced by upstream tools
        if find "$RUN_DIR" -type f \
             \( -path '*/cell_selection/labels.tsv' -o -path '*/predictions/labels.tsv' -o -path '*/prediction/labels.tsv' \) \
             -print -quit | grep -q .; then

          TMP="{output.pred}.tmp"
          find "$RUN_DIR" -type f \
            \( -path '*/cell_selection/labels.tsv' -o -path '*/predictions/labels.tsv' -o -path '*/prediction/labels.tsv' \) \
            -print | sort \
            | xargs awk 'FNR==1 && NR>1 {{next}} {{print}}' > "$TMP"

          test -s "$TMP"
          mv -f "$TMP" "{output.pred}"
          exit 0
        fi

        # Fallback: run predict from features.tsv
        python -u "{params.bin}" -j {threads} predict \
          -p "{input.features}" \
          -o "{output.pred_dir}" \
          -m "{params.model}"

        test -s "{output.pred}"
        """


# Final merge (R) with normalized Ashley tables; copy to results/
rule merge_final_qc_with_ashleys:
    input:
        final = out_path("final_qc.tsv"),
        pred  = out_path("ashleys/prediction/prediction.tsv"),
        feat  = out_path("ashleys/features.tsv")
    output:
        merged = out_path("final_qc_with_ashleys.tsv")
    log:
        out_path("logs/merge_final_qc_with_ashleys.log")
    conda:
        "envs/R_merge.yaml"
    shell:
        r"""
        set -euo pipefail
        mkdir -p "$(dirname {log})" "$(dirname {output.merged})"

        FEAT_ARG=""
        [[ -s "{input.feat}" ]] && FEAT_ARG="--feat {input.feat}"

        # Write directly to the declared output path; no copies afterwards.
        Rscript workflow/scripts/merge_ashleys.R \
          --final "{input.final}" \
          --pred  "{input.pred}" \
          ${{FEAT_ARG}} \
          --out  "{output.merged}" \
          > "{log}" 2>&1
        """
