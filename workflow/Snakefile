# Import necessary modules
import os
import glob
import gzip
import subprocess
import yaml
import pandas as pd
from utils import extract_alignment_metrics

#ASCII art for BamQC-BIH
print(r"""


 /$$$$$$$                           /$$$$$$   /$$$$$$        /$$$$$$$  /$$$$$$ /$$   /$$
| $$__  $$                         /$$__  $$ /$$__  $$      | $$__  $$|_  $$_/| $$  | $$
| $$  \ $$  /$$$$$$  /$$$$$$/$$$$ | $$  \ $$| $$  \__/      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$  |____  $$| $$_  $$_  $$| $$  | $$| $$            | $$$$$$$   | $$  | $$$$$$$$
| $$__  $$  /$$$$$$$| $$ \ $$ \ $$| $$  | $$| $$            | $$__  $$  | $$  | $$__  $$
| $$  \ $$ /$$__  $$| $$ | $$ | $$| $$/$$ $$| $$    $$      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$/|  $$$$$$$| $$ | $$ | $$|  $$$$$$/|  $$$$$$/      | $$$$$$$/ /$$$$$$| $$  | $$
|_______/  \_______/|__/ |__/ |__/ \____ $$$ \______/       |_______/ |______/|__/  |__/
                                        \__/                                                       

""")

# Load config
configfile: "config/config.yaml"

# Define input and output directories
data_location = config["data_location"]
output_location = config["output_location"]
ref = config["ref"]
reference_path = config["reference_path"]

# Print summary of parameters
print("Pipeline launched with the following parameters:")
print(f"- Data Location: \033[1;31m{data_location}\033[0m")
print(f"- Output Location: \033[1;32m{output_location}\033[0m")
print(f"- Reference Genome: \033[1;33m{ref}\033[0m")
print("")

# Define the file path to the reference genomes
reference_file = os.path.join(reference_path, f"{ref}.fa")

# Define the list of samples based on the input BAM files
samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]

# Rule to generate all outputs for the pipeline
rule all:
    input:
        os.path.join(output_location, "summary_alignment_metrics.tsv"),
        os.path.join(output_location, "read_length_distributions.tsv"),
        os.path.join(output_location, "mean_base_quality.tsv"),
        os.path.join(output_location, "base_content.tsv"),
        os.path.join(output_location, "mapping_quality.tsv"),
        os.path.join(output_location, "coverage_distributions.tsv"),
        os.path.join(output_location, "chromosomal_mappings.tsv"),
        os.path.join(output_location, "insert_sizes.tsv"),
        os.path.join(output_location, "indel_context.tsv"),
        os.path.join(output_location, "indel_sizes.tsv"),
        os.path.join(output_location, "clipping.tsv"),
        os.path.join(output_location, "gc_content.tsv")

# Print the expand function to debug
print("Expanding input files with the following pattern:")
print(os.path.join(config['output_location'], "{sample}.qc.tsv.gz"))
print("")

# Rule to apply Alfred QC pipeline to each BAM file individually
rule alfred_qc:
    input:
        bam = os.path.join(data_location, "{sample}.sort.mdup.bam"),
        reference = reference_file
    output:
        qc = os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz")
    conda:
        "envs/qc_env.yaml"
    shell:
        """
        alfred qc -r {input.reference} -o {output.qc} {input.bam}
        """

rule extract_summary_metrics:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        summary = os.path.join(output_location, "summary_alignment_metrics.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    run:
        # Use zgrep and cut to extract alignment summary metrics into a temporary file
        tmp_file = output[0].replace(".tsv", "_tmp.tsv")
        zgrep_cut_cmd = f"zgrep ^ME {input} | cut -f 2- > {tmp_file}"
        shell(zgrep_cut_cmd)
        
        # Initialize a DataFrame to store aggregated metrics
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_alignment_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.summary), mode='a')

        # Remove the temporary file
        os.remove(tmp_file)

rule extract_readlength_distributions:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        RL = os.path.join(output_location, "read_length_distributions.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract read length distributions into a temporary file
        zgrep ^RL {input} | cut -f 2- > {output.RL}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.RL}.tmp > {output.RL}
        
        # Clean up temporary file
        rm {output.RL}.tmp
        """

# Rule to extract mean Base Quality distributions
rule extract_mean_base_quality:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        BQ = os.path.join(output_location, "mean_base_quality.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract mean base quality distributions into a temporary file
        zgrep ^BQ {input} | cut -f 2- > {output.BQ}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.BQ}.tmp > {output.BQ}
        
        # Clean up temporary file
        rm {output.BQ}.tmp
        """

# Rule to extract Base Composition distributions
rule extract_base_content:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        BC = os.path.join(output_location, "base_content.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract base content distributions into a temporary file
        zgrep ^BC {input} | cut -f 2- > {output.BC}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.BC}.tmp > {output.BC}
        
        # Clean up temporary file
        rm {output.BC}.tmp
        """

# Rule to extract Mapping Quality
rule extract_mapping_quality:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        MQ = os.path.join(output_location, "mapping_quality.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract mapping quality distributions into a temporary file
        zgrep ^MQ {input} | cut -f 2- > {output.MQ}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.MQ}.tmp > {output.MQ}
        
        # Clean up temporary file
        rm {output.MQ}.tmp
        """

# Rule to extract Coverage distributions
rule extract_coverage:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        CO = os.path.join(output_location, "coverage_distributions.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract coverage distributions into a temporary file
        zgrep ^CO {input} | cut -f 2- > {output.CO}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.CO}.tmp > {output.CO}
        
        # Clean up temporary file
        rm {output.CO}.tmp
        """

# Rule to extract Chromosomal Mapping statistics
rule extract_chromosomal_mapping:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        CM = os.path.join(output_location, "chromosomal_mappings.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract chromosomal mapping distributions into a temporary file
        zgrep ^CM {input} | cut -f 2- > {output.CM}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.CM}.tmp > {output.CM}
        
        # Clean up temporary file
        rm {output.CM}.tmp
        """

# Rule to extract Insert Size distributions
rule extract_insert_size:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        IS = os.path.join(output_location, "insert_sizes.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract insert size distributions into a temporary file
        zgrep ^IS {input} | cut -f 2- > {output.IS}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.IS}.tmp > {output.IS}
        
        # Clean up temporary file
        rm {output.IS}.tmp
        """

# Rule to extract InDel context distributions
rule extract_insert_complexity:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        IC = os.path.join(output_location, "indel_context.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract InDel contesxt into a temporary file
        zgrep ^IC {input} | cut -f 2- > {output.IC}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.IC}.tmp > {output.IC}
        
        # Clean up temporary file
        rm {output.IC}.tmp
        """

# Rule to extract InDel sizes
rule extract_indel_sizes:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        IZ = os.path.join(output_location, "indel_sizes.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract InDel sizes  into a temporary file
        zgrep ^IZ {input} | cut -f 2- > {output.IZ}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.IZ}.tmp > {output.IZ}
        
        # Clean up temporary file
        rm {output.IZ}.tmp
        """

# Rule to extract Clipping distributions
rule extract_clipping:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        CG = os.path.join(output_location, "clipping.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract clipping into a temporary file
        zgrep ^CG {input} | cut -f 2- > {output.CG}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.CG}.tmp > {output.CG}
        
        # Clean up temporary file
        rm {output.CG}.tmp
        """

# Rule to extract GC Content distributions
rule extract_gc_content:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        GC = os.path.join(output_location, "gc_content.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract GC content by sample into a temporary file
        zgrep ^GC {input} | cut -f 2- > {output.GC}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.GC}.tmp > {output.GC}
        
        # Clean up temporary file
        rm {output.GC}.tmp
        """
