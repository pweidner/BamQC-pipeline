# Import necessary modules
import os
import glob
import subprocess
import yaml
import pandas as pd
from utils import extract_alignment_metrics

# ASCII art for BamQC-BIH
print(r"""
 /$$$$$$$                           /$$$$$$   /$$$$$$        /$$$$$$$  /$$$$$$ /$$   /$$
| $$__  $$                         /$$__  $$ /$$__  $$      | $$__  $$|_  $$_/| $$  | $$
| $$  \ $$  /$$$$$$  /$$$$$$/$$$$ | $$  \ $$| $$  \__/      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$  |____  $$| $$_  $$_  $$| $$  | $$| $$            | $$$$$$$   | $$  | $$$$$$$$
| $$__  $$  /$$$$$$$| $$ \ $$ \ $$| $$  | $$| $$            | $$__  $$  | $$  | $$__  $$
| $$  \ $$ /$$__  $$| $$ | $$ | $$| $$/$$ $$| $$    $$      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$/|  $$$$$$$| $$ | $$ | $$|  $$$$$$/|  $$$$$$/      | $$$$$$$/ /$$$$$$| $$  | $$
|_______/  \_______/|__/ |__/ |__/ \____ $$$ \______/       |_______/ |______/|__/  |__/
                                        \__/                                                       
""")

# Load config file
configfile: "config/config.yaml"

# Define input and output directories, reference, etc.
data_location = config["data_location"]
output_location = config["output_location"]
ref = config["ref"]
reference_path = config["reference_path"]

# Print summary of parameters with color coding for clarity
print("Pipeline launched with the following parameters:")
print(f"- Data Location: \033[1;31m{data_location}\033[0m")
print(f"- Output Location: \033[1;32m{output_location}\033[0m")
print(f"- Reference Genome: \033[1;33m{ref}\033[0m")
print("")

# Define the file path to the reference genome file
reference_file = os.path.join(reference_path, f"{ref}")

# Define the list of samples based on the input BAM files.
# Using rsplit to handle cases where filenames contain multiple periods.
samples = [os.path.basename(f).rsplit(".sort.mdup", 1)[0] 
           for f in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]

# Print the expanded pattern for debugging purposes
print("Expanding input files with the following pattern:")
print(os.path.join(config['output_location'], "stats-by-lib", "{sample}.qc.tsv.gz"))
print("")


#########################################
# Rule: all
# Final target rule that collects the cleaned alignment summary metrics.
#########################################
rule all:
    """
    Collect all final output files
    """
    input:    
        os.path.join(output_location, "alignment_summary_metrics_cleaned.tsv")


#########################################
# Rule: alfred_qc
# Process each BAM file with Alfred to generate QC statistics.
#########################################
rule alfred_qc:
    input:
        bam = os.path.join(data_location, "{sample}.sort.mdup.bam"),
        reference = reference_file
    output:
        qc = os.path.join(output_location, "stats-by-lib", "{sample}.qc.tsv.gz")
    conda:
        "envs/qc_env.yaml"
    threads: 4  # Allocate additional threads for processing if needed.
    log:
        os.path.join(output_location, "logs", "{sample}.alfred_qc.log")
    shell:
        """
        # Create log directory if it doesn't exist
        mkdir -p {os.path.join(output_location, "logs")}
        echo "Processing sample: {wildcards.sample}" > {log}
        alfred qc -r {input.reference} -o {output.qc} {input.bam} >> {log} 2>&1
        """


#########################################
# Rule: aggregate_alignment_metrics
# Collect, process, and aggregate the QC metrics from all sample files.
#########################################
rule aggregate_alignment_metrics:
    input:
        qc_files = expand(os.path.join(output_location, "stats-by-lib", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        summary = os.path.join(output_location, "alignment_summary_metrics.tsv")
    threads: 2
    run:
        import pandas as pd
        import os

        # Define a temporary file for storing intermediate zgrep output
        tmp_file = output.summary.replace(".tsv", "_tmp.tsv")
        
        # Process each QC file using a shell loop.
        # This avoids issues with passing a list of files in a single command.
        shell("""
            for f in {input.qc_files}; do
                zgrep '^ME' "$f" | cut -f2-;
            done > {tmp_file}
        """)
        
        # Initialize an empty DataFrame for aggregated metrics
        df = pd.DataFrame()
        
        # Process each QC file individually using the helper function.
        for qc_file in input.qc_files:
            try:
                sample_metrics = extract_alignment_metrics(qc_file)
            except Exception as e:
                print(f"Error processing {qc_file}: {e}")
                continue
            
            # Convert the extracted metrics dictionary to a DataFrame row.
            # Transposing ensures one row per sample.
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index").T
            df = pd.concat([df, sample_df], ignore_index=True)
        
        # Write the aggregated results to the summary file.
        df.to_csv(output.summary, sep='\t', index=False)
        
        # Clean up the temporary file if it exists.
        if os.path.exists(tmp_file):
            os.remove(tmp_file)


#########################################
# Rule: clean_up_table
# Remove the header row from the raw aggregated metrics file and update it.
#########################################
rule clean_up_table:
    input:
        raw_metrics = os.path.join(output_location, "alignment_summary_metrics.tsv")
    output:
        cleaned_metrics = os.path.join(output_location, "alignment_summary_metrics_cleaned.tsv")
    shell:
        """
        # Remove the header from the raw metrics file and overwrite the original file
        tail -n +2 {input.raw_metrics} > {output.cleaned_metrics} && mv {output.cleaned_metrics} {input.raw_metrics}
        """
