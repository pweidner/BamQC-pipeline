configfile: "config/config.yaml"

# ================================= ASCII Art =================================
print(r"""
 /$$$$$$$                           /$$$$$$   /$$$$$$        /$$$$$$$  /$$$$$$ /$$   /$$
| $$__  $$                         /$$__  $$ /$$__  $$      | $$__  $$|_  $$_/| $$  | $$
| $$  \ $$  /$$$$$$  /$$$$$$/$$$$ | $$  \ $$| $$  \__/      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$  |____  $$| $$_  $$_  $$| $$  | $$| $$            | $$$$$$$   | $$  | $$$$$$$$
| $$__  $$  /$$$$$$$| $$ \ $$ \ $$| $$  | $$| $$            | $$__  $$  | $$  | $$__  $$
| $$  \ $$ /$$__  $$| $$ | $$ | $$| $$/$$ $$| $$    $$      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$/|  $$$$$$$| $$ | $$ | $$|  $$$$$$/|  $$$$$$/      | $$$$$$$/ /$$$$$$| $$  | $$
|_______/  \_______/|__/ |__/ |__/ \____ $$$ \______/       |_______/ |______/|__/  |__/
                                        \__/                                                       
""")
print("Starting BamQC pipeline...\n")

# -- Configuration & Paths --
import os
import glob

DATA_DIR      = config["data_location"]
OUT_DIR       = config["output_location"]
REF_NAME      = config["ref"]
REF_PATH      = config["reference_path"]
# Construct full path to reference FASTA
REF_FILE      = os.path.join(REF_PATH, f"{REF_NAME}.fa")

LOG_DIR       = os.path.join(OUT_DIR, "logs")
STATS_DIR     = os.path.join(OUT_DIR, "stats-by-lib")
ANEUF_OUT_DIR = os.path.join(OUT_DIR, "aneufinder-results")

# -- Verbose parameter summary --
print("Pipeline launched with the following parameters:")
print(f"- Data Location: \033[1;31m{DATA_DIR}\033[0m")
print(f"- Output Location: \033[1;32m{OUT_DIR}\033[0m")
print(f"- Reference Genome: \033[1;33m{REF_NAME} (fasta: {REF_FILE})\033[0m")
print("")

# -- Derive sample names at parse time --
SAMPLES = [
    os.path.basename(f).replace(".sort.mdup.bam", "")
    for f in glob.glob(os.path.join(DATA_DIR, "*.sort.mdup.bam"))
]

# -- Verbose expansion info --
print("Expanding input files with the following pattern:")
print(os.path.join(STATS_DIR, "{sample}.qc.tsv.gz"))
print("")

# -- Rule all: aggregate final outputs --
rule all:
    input:
        os.path.join(OUT_DIR, "alignment_summary_metrics.tsv"),
        os.path.join(OUT_DIR, "aggregate_mosaicatcher_qc.tsv"),
        os.path.join(ANEUF_OUT_DIR, "done.txt")

# -- Rule: Alfred QC per sample --
rule alfred_qc:
    input:
        bam=lambda wc: os.path.join(DATA_DIR, f"{wc.sample}.sort.mdup.bam"),
        ref=lambda: REF_FILE
    output:
        qc=os.path.join(STATS_DIR, "{sample}.qc.tsv.gz")
    conda:
        "envs/qc_env.yaml"
    threads: 4
    log:
        os.path.join(LOG_DIR, "{sample}.alfred_qc.log")
    shell:
        """
        mkdir -p {LOG_DIR} {STATS_DIR}
        echo "Processing sample: {wildcards.sample}" > {log}
        alfred qc -r {input.ref} -o {output.qc} {input.bam} >> {log} 2>&1
        """

# -- Rule: Aneufinder QC on entire folder --
rule run_aneufinder:
    input:
        bam_folder=DATA_DIR
    output:
        marker=os.path.join(ANEUF_OUT_DIR, "done.txt")
    conda:
        "envs/aneufinder_env.yaml"
    threads: 2
    log:
        os.path.join(LOG_DIR, "aneufinder_run.log")
    shell:
        """
        mkdir -p {LOG_DIR} {ANEUF_OUT_DIR}
        Rscript workflow/scripts/aneufinder_run.R \
            --inputfolder {input.bam_folder} \
            --outputfolder {ANEUF_OUT_DIR} \
            --nCPU {threads} >> {log} 2>&1
        touch {output.marker}
        """

# -- Rule: Aggregate Alfred alignment metrics --
rule aggregate_alignment_metrics:
    input:
        qc_files=expand(os.path.join(STATS_DIR, "{sample}.qc.tsv.gz"), sample=SAMPLES)
    output:
        summary=os.path.join(OUT_DIR, "alignment_summary_metrics.tsv")
    threads: 2
    run:
        import pandas as pd
        import gzip

        records = []
        for qc in input.qc_files:
            sample_id = os.path.basename(qc).replace(".qc.tsv.gz", "")
            header, values = None, None
            with gzip.open(qc, 'rt') as fh:
                for line in fh:
                    if line.startswith("ME"):
                        tokens = line.strip().split("\t")
                        if header is None:
                            header = tokens[1:]
                        else:
                            values = tokens[1:]
                            break
            if header and values:
                rec = dict(zip(header, values))
                rec["Library"] = sample_id
                records.append(rec)
            else:
                print(f"Warning: could not parse QC for {qc}")

        df = pd.DataFrame(records)
        cols = [c for c in df.columns if c != "Library"]
        df = df[["Library"] + cols]
        df.to_csv(output.summary, sep="\t", index=False)

# -- Rule: Aggregate Mosaicatcher QC & merge with Alfred metrics --
rule aggregate_mosaicatcher_qc:
    input:
        alfred_summary=os.path.join(OUT_DIR, "alignment_summary_metrics.tsv")
    output:
        mosaic_qc=os.path.join(OUT_DIR, "aggregate_mosaicatcher_qc.tsv")
    run:
        import pandas as pd
        import glob

        parent_dir = os.path.dirname(DATA_DIR)
        labels_file = os.path.join(parent_dir, "cell_selection", "labels.tsv")
        counts_dir  = os.path.join(parent_dir, "counts")

        df_labels = pd.read_csv(labels_file, sep="\t")
        df_labels['cell'] = df_labels['cell'].str.replace(r"\.sort\.mdup\.bam$", '', regex=True)

        info_files = glob.glob(os.path.join(counts_dir, "*.info"))
        df_counts = pd.concat((pd.read_csv(f, sep="\t", comment='#') for f in info_files), ignore_index=True)

        df_alfred = pd.read_csv(input.alfred_summary, sep="\t")

        df = df_labels.merge(df_counts, on=["cell", "sample"], how="left")
        df = df.merge(df_alfred, left_on=["cell", "sample"], right_on=["Library", "Library"], how="left")
        df.drop(columns=["Library"], inplace=True)

        df.to_csv(output.mosaic_qc, sep="\t", index=False)
