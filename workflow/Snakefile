# Import necessary modules
import os
import glob
import gzip
import subprocess
import yaml
import pandas as pd
from utils import extract_alignment_metrics

#ASCII art for BamQC-BIH
print(r"""


 /$$$$$$$                           /$$$$$$   /$$$$$$        /$$$$$$$  /$$$$$$ /$$   /$$
| $$__  $$                         /$$__  $$ /$$__  $$      | $$__  $$|_  $$_/| $$  | $$
| $$  \ $$  /$$$$$$  /$$$$$$/$$$$ | $$  \ $$| $$  \__/      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$  |____  $$| $$_  $$_  $$| $$  | $$| $$            | $$$$$$$   | $$  | $$$$$$$$
| $$__  $$  /$$$$$$$| $$ \ $$ \ $$| $$  | $$| $$            | $$__  $$  | $$  | $$__  $$
| $$  \ $$ /$$__  $$| $$ | $$ | $$| $$/$$ $$| $$    $$      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$/|  $$$$$$$| $$ | $$ | $$|  $$$$$$/|  $$$$$$/      | $$$$$$$/ /$$$$$$| $$  | $$
|_______/  \_______/|__/ |__/ |__/ \____ $$$ \______/       |_______/ |______/|__/  |__/
                                        \__/                                                       

""")

# Load config
configfile: "config/config.yaml"

# Define input and output directories
data_location = config["data_location"]
output_location = config["output_location"]
ref = config["ref"]
reference_path = config["reference_path"]

# Print summary of parameters
print("Pipeline launched with the following parameters:")
print(f"- Data Location: \033[1;31m{data_location}\033[0m")
print(f"- Output Location: \033[1;32m{output_location}\033[0m")
print(f"- Reference Genome: \033[1;33m{ref}\033[0m")
print("")

# Define the file path to the reference genomes
reference_file = os.path.join(reference_path, f"{ref}.fa")

# Define the list of samples based on the input BAM files
samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]

# Rule to generate a list of BAM files
rule all:
    """
    Collect all sample QC files as final outputs
    """ 
    input:    
        os.path.join(output_location, "summary_metrics.tsv"),
        os.path.join(output_location, "readlength_distributions.tsv")

# Print the expand function to debug
print("Expanding input files with the following pattern:")
print(os.path.join(config['output_location'], "{sample}.qc.tsv.gz"))
print("")

# Rule to apply Alfred QC pipeline to each BAM file individually
rule alfred_qc:
    input:
        bam = os.path.join(data_location, "{sample}.sort.mdup.bam"),
        reference = reference_file
    output:
        qc = os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz")
    conda:
        "envs/qc_env.yaml"
    shell:
        """
        alfred qc -r {input.reference} -o {output.qc} {input.bam}
        """

rule extract_summary_metrics:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        summary = os.path.join(output_location, "summary_metrics.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    run:
        # Use zgrep and cut to extract alignment summary metrics into a temporary file
        tmp_file = output[0].replace(".tsv", "_tmp.tsv")
        zgrep_cut_cmd = f"zgrep ^ME {input} | cut -f 2- > {tmp_file}"
        shell(zgrep_cut_cmd)
        
        # Initialize a DataFrame to store aggregated metrics
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_alignment_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.summary), mode='a')

        # Remove the temporary file
        os.remove(tmp_file)

rule extract_readlength_distributions:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        RL = os.path.join(output_location, "readlength_distributions.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    shell:
        """
        # Use zgrep and cut to extract read length distributions into a temporary file
        zgrep ^RL {input} | cut -f 2- > {output.RL}.tmp
        
        # Remove duplicate headers from the temporary file
        awk 'NR==1 || $0 != header {{header=$0; print}}' {output.RL}.tmp > {output.RL}
        
        # Clean up temporary file
        rm {output.RL}.tmp
        """
