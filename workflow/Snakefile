# Import necessary modules
import os
import glob
import gzip
import subprocess
import yaml
import pandas as pd
from utils import extract_metrics

#ASCII art for BamQC-BIH
print(r"""


 /$$$$$$$                           /$$$$$$   /$$$$$$        /$$$$$$$  /$$$$$$ /$$   /$$
| $$__  $$                         /$$__  $$ /$$__  $$      | $$__  $$|_  $$_/| $$  | $$
| $$  \ $$  /$$$$$$  /$$$$$$/$$$$ | $$  \ $$| $$  \__/      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$  |____  $$| $$_  $$_  $$| $$  | $$| $$            | $$$$$$$   | $$  | $$$$$$$$
| $$__  $$  /$$$$$$$| $$ \ $$ \ $$| $$  | $$| $$            | $$__  $$  | $$  | $$__  $$
| $$  \ $$ /$$__  $$| $$ | $$ | $$| $$/$$ $$| $$    $$      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$/|  $$$$$$$| $$ | $$ | $$|  $$$$$$/|  $$$$$$/      | $$$$$$$/ /$$$$$$| $$  | $$
|_______/  \_______/|__/ |__/ |__/ \____ $$$ \______/       |_______/ |______/|__/  |__/
                                        \__/                                                       

""")

# Load config
configfile: "config/config.yaml"

# Define input and output directories
data_location = config["data_location"]
output_location = config["output_location"]
ref = config["ref"]
reference_path = config["reference_path"]

# Print summary of parameters
print("Pipeline launched with the following parameters:")
print(f"- Data Location: \033[1;31m{data_location}\033[0m")
print(f"- Output Location: \033[1;32m{output_location}\033[0m")
print(f"- Reference Genome: \033[1;33m{ref}\033[0m")
print("")

# Define the file path to the reference genomes
reference_file = os.path.join(reference_path, f"{ref}.fa")

# Define the list of samples based on the input BAM files
samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]

# Rule to generate all outputs for the pipeline
rule all:
    input:
        os.path.join(output_location, "summary_alignment_metrics.tsv"),
        os.path.join(output_location, "read_length_distributions.tsv"),
        os.path.join(output_location, "mean_base_quality.tsv"),
        os.path.join(output_location, "base_content.tsv"),
        os.path.join(output_location, "mapping_quality.tsv"),
        os.path.join(output_location, "coverage_distributions.tsv"),
        os.path.join(output_location, "chromosomal_mappings.tsv"),
        os.path.join(output_location, "insert_sizes.tsv"),
        os.path.join(output_location, "indel_context.tsv"),
        os.path.join(output_location, "indel_sizes.tsv"),
        os.path.join(output_location, "clipping.tsv"),
        os.path.join(output_location, "gc_content.tsv")

# Print the expand function to debug
print("Expanding input files with the following pattern:")
print(os.path.join(config['output_location'], "{sample}.qc.tsv.gz"))
print("")

# Rule to apply Alfred QC pipeline to each BAM file individually
rule alfred_qc:
    input:
        bam = os.path.join(data_location, "{sample}.sort.mdup.bam"),
        reference = reference_file
    output:
        qc = os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz")
    conda:
        "envs/qc_env.yaml"
    shell:
        """
        alfred qc -r {input.reference} -o {output.qc} {input.bam}
        """

rule get_summary_metrics:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        summary = os.path.join(output_location, "summary_alignment_metrics.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "MD"
    run:       
        # Initialize a DataFrame to store aggregated metrics
        row_id = "MD"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.summary), mode='a')

rule get_readlength_distributions:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        RL = os.path.join(output_location, "read_length_distributions.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "MD"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "MD"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.RL), mode='a')

# Rule to extract mean Base Quality distributions
rule get_mean_base_quality:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        BQ = os.path.join(output_location, "mean_base_quality.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "BQ"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "BQ"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.BQ), mode='a')


# Rule to extract Base Composition distributions
rule get_base_content:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        BC = os.path.join(output_location, "base_content.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "BC"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "BC"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.BC), mode='a')

# Rule to extract Mapping Quality
rule get_mapping_quality:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        MQ = os.path.join(output_location, "mapping_quality.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "MQ"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "MQ"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.MQ), mode='a')

# Rule to extract Coverage distributions
rule get_coverage:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        CO = os.path.join(output_location, "coverage_distributions.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "CO"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "CO"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.CO), mode='a')

# Rule to extract Chromosomal Mapping statistics
rule get_chromosomal_mapping:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        CM = os.path.join(output_location, "chromosomal_mappings.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "CM"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "CM"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.CM), mode='a')

# Rule to extract Insert Size distributions
rule get_insert_size:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        IS = os.path.join(output_location, "insert_sizes.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "IS"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "IS"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.IS), mode='a')

# Rule to extract InDel context distributions
rule get_insert_complexity:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        IC = os.path.join(output_location, "indel_context.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "IC"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "IC"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.IC), mode='a')
# Rule to extract InDel sizes
rule get_indel_sizes:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        IZ = os.path.join(output_location, "indel_sizes.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "IZ"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "IZ"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.IZ), mode='a')

# Rule to extract Clipping distributions
rule get_clipping:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        CG = os.path.join(output_location, "clipping.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "CG"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "CG"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.CG), mode='a')

# Rule to extract GC Content distributions
rule get_gc_content:
    input:
        expand(os.path.join(output_location, "metrics-per-sample", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        GC = os.path.join(output_location, "gc_content.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))],
        row_id = "GC"
    run:
        # Initialize a DataFrame to store aggregated metrics
        row_id = "GC"
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.GC), mode='a')
