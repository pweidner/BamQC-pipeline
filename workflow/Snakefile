# Import necessary modules
import os
import glob
import subprocess
import yaml
import pandas as pd
from utils import extract_alignment_metrics

#ASCII art for BamQC-BIH
print(r"""


 /$$$$$$$                           /$$$$$$   /$$$$$$        /$$$$$$$  /$$$$$$ /$$   /$$
| $$__  $$                         /$$__  $$ /$$__  $$      | $$__  $$|_  $$_/| $$  | $$
| $$  \ $$  /$$$$$$  /$$$$$$/$$$$ | $$  \ $$| $$  \__/      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$  |____  $$| $$_  $$_  $$| $$  | $$| $$            | $$$$$$$   | $$  | $$$$$$$$
| $$__  $$  /$$$$$$$| $$ \ $$ \ $$| $$  | $$| $$            | $$__  $$  | $$  | $$__  $$
| $$  \ $$ /$$__  $$| $$ | $$ | $$| $$/$$ $$| $$    $$      | $$  \ $$  | $$  | $$  | $$
| $$$$$$$/|  $$$$$$$| $$ | $$ | $$|  $$$$$$/|  $$$$$$/      | $$$$$$$/ /$$$$$$| $$  | $$
|_______/  \_______/|__/ |__/ |__/ \____ $$$ \______/       |_______/ |______/|__/  |__/
                                        \__/                                                       

""")

# Load config
configfile: "config/config.yaml"

# Define input and output directories
data_location = config["data_location"]
output_location = config["output_location"]
ref = config["ref"]
reference_path = config["reference_path"]

# Print summary of parameters
print("Pipeline launched with the following parameters:")
print(f"- Data Location: \033[1;31m{data_location}\033[0m")
print(f"- Output Location: \033[1;32m{output_location}\033[0m")
print(f"- Reference Genome: \033[1;33m{ref}\033[0m")
print("")

# Define the file path to the reference genomes
reference_file = os.path.join(reference_path, f"{ref}.fa")

# Define the list of samples based on the input BAM files
samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]

# Rule to generate a list of BAM files
rule all:
    """
    Collect all sample QC files as final outputs
    """ 
    input:    
        os.path.join(output_location, "alignment_summary_metrics.tsv"),
        os.path.join(output_location, "RL_metrics.tsv"),
        os.path.join(output_location, "BQ_metrics.tsv"),
        os.path.join(output_location, "BC_metrics.tsv"),
        os.path.join(output_location, "MQ_metrics.tsv"),
        os.path.join(output_location, "CO_metrics.tsv"),
        os.path.join(output_location, "CM_metrics.tsv"),
        os.path.join(output_location, "IS_metrics.tsv"),
        os.path.join(output_location, "IC_metrics.tsv"),
        os.path.join(output_location, "IZ_metrics.tsv"),
        os.path.join(output_location, "CG_metrics.tsv"),
        os.path.join(output_location, "GC_metrics.tsv")

# Print the expand function to debug
print("Expanding input files with the following pattern:")
print(os.path.join(config['output_location'], "{sample}.qc.tsv.gz"))
print("")

# Rule to apply Alfred QC pipeline to each BAM file individually
rule alfred_qc:
    input:
        bam = os.path.join(data_location, "{sample}.sort.mdup.bam"),
        reference = reference_file
    output:
        qc = os.path.join(output_location, "stats-by-lib", "{sample}.qc.tsv.gz")
    conda:
        "envs/qc_env.yaml"
    shell:
        """
        alfred qc -r {input.reference} -o {output.qc} {input.bam}
        """

rule aggregate_alignment_metrics:
    input:
        expand(os.path.join(output_location, "stats-by-lib", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        summary = os.path.join(output_location, "alignment_summary_metrics.tsv")
    params:
        samples = [os.path.basename(sample).split(".")[0] for sample in glob.glob(os.path.join(data_location, "*.sort.mdup.bam"))]
    run:
        # Use zgrep and cut to extract alignment summary metrics into a temporary file
        tmp_file = output[0].replace(".tsv", "_tmp.tsv")
        zgrep_cut_cmd = f"zgrep ^ME {input} | cut -f 2- > {tmp_file}"
        shell(zgrep_cut_cmd)
        
        # Initialize a DataFrame to store aggregated metrics
        df = None
        # Iterate over individual sample files
        for sample_file in input:
            # Extract alignment summary metrics for the current sample
            sample_metrics = extract_alignment_metrics(sample_file)
            sample_df = pd.DataFrame.from_dict(sample_metrics, orient="index")

            # Append the sample DataFrame to the main DataFrame
            if df is None:
                df = sample_df
            else:
                # Append the second row of the DataFrame
                df = pd.concat([df, sample_df.iloc[1:]]) 
        
        # Write the aggregated metrics to the output file
        df.to_csv(output.summary, sep='\t', index=False, header=not os.path.exists(output.summary), mode='a')

        # Remove the temporary file
        os.remove(tmp_file)

# Rule to extract tables for each metric from Alfred QC output files
rule extract_metrics_tables:
    input:
        expand(os.path.join(output_location, "stats-by-lib", "{sample}.qc.tsv.gz"), sample=samples)
    output:
        RL_metrics = os.path.join(output_location, "RL_metrics.tsv"),
        BQ_metrics = os.path.join(output_location, "BQ_metrics.tsv"),
        BC_metrics = os.path.join(output_location, "BC_metrics.tsv"),
        MQ_metrics = os.path.join(output_location, "MQ_metrics.tsv"),
        CO_metrics = os.path.join(output_location, "CO_metrics.tsv"),
        CM_metrics = os.path.join(output_location, "CM_metrics.tsv"),
        IS_metrics = os.path.join(output_location, "IS_metrics.tsv"),
        IC_metrics = os.path.join(output_location, "IC_metrics.tsv"),
        IZ_metrics = os.path.join(output_location, "IZ_metrics.tsv"),
        CG_metrics = os.path.join(output_location, "CG_metrics.tsv"),
        GC_metrics = os.path.join(output_location, "GC_metrics.tsv")
    run:
        # Initialize dictionaries to store metrics data
        metrics_data = {
            "RL_metrics": [],
            "BQ_metrics": [],
            "BC_metrics": [],
            "MQ_metrics": [],
            "CO_metrics": [],
            "CM_metrics": [],
            "IS_metrics": [],
            "IC_metrics": [],
            "IZ_metrics": [],
            "CG_metrics": [],
            "GC_metrics": []
        }

        # Define the zgrep and cut commands for each metric
        commands = {
            "RL_metrics_cmd": "zgrep '^RL' {} | cut -f 2-",
            "BQ_metrics_cmd": "zgrep '^BQ' {} | cut -f 2-",
            "BC_metrics_cmd": "zgrep '^BC' {} | cut -f 2-",
            "MQ_metrics_cmd": "zgrep '^MQ' {} | cut -f 2-",
            "CO_metrics_cmd": "zgrep '^CO' {} | cut -f 2-",
            "CM_metrics_cmd": "zgrep '^CM' {} | cut -f 2-",
            "IS_metrics_cmd": "zgrep '^IS' {} | cut -f 2-",
            "IC_metrics_cmd": "zgrep '^IC' {} | cut -f 2-",
            "IZ_metrics_cmd": "zgrep '^IZ' {} | cut -f 2-",
            "CG_metrics_cmd": "zgrep '^CG' {} | cut -f 2-",
            "GC_metrics_cmd": "zgrep '^GC' {} | cut -f 2-"
        }
        
        # Iterate over input files
        for infile in input:
            # Iterate over commands and execute them
            for key, command in commands.items():
                # Execute the command and capture the output
                output = subprocess.check_output(command.format(infile), shell=True).decode("utf-8").strip().split('\n')
                # If the header hasn't been written yet, write it to the output file
                if not metrics_data[key]:
                    metrics_data[key].extend(output)
                else:
                    # Append the data (excluding the header) to the output file
                    metrics_data[key].extend(output[1:])
        
        # Write the metrics data to the output files
        for key, data in metrics_data.items():
            with open(output[key], 'w') as f:
                f.write('\n'.join(data))
